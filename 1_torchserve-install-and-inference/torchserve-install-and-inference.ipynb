{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With TorchServe\n",
    "\n",
    "## Install PyTorch and TorchServe and conduct an inference.\n",
    "\n",
    "This introductory lab will prepare your environment and test that TorchServe is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the TorchServe environment on SageMaker\n",
    "1. Use Amazon Corretto to install Java 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo rpm --import https://yum.corretto.aws/corretto.key \n",
    "sudo curl -L -o /etc/yum.repos.d/corretto.repo https://yum.corretto.aws/corretto.repo\n",
    "sudo yum install -y java-11-amazon-corretto-devel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Verify the Java version and that JAVA_HOME is properly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version\n",
    "!echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use pip to install TorchServe and the model archiver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchtext torchvision sentencepiece psutil future\n",
    "!pip install torchserve torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If the serve subdirectory already exists, remove it. And, then clone the TorchServe repository into a new serve subdirectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d \"serve\" ]; then\n",
    "    rm -r -f serve\n",
    "fi\n",
    "git clone https://github.com/pytorch/serve.git serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create thde model_store directory which is reference via a parameter by the model_archiver. If the model_store subdirectory already exists, remove all the files. Else, create a subdirectory in which to store your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d \"model_store\" ]; then\n",
    "    rm -f model_store/*    \n",
    "else\n",
    "    mkdir model_store\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Download a trained model. The DenseNet model is one of the PyTorch TorchVision [models](https://pytorch.org/docs/stable/torchvision/models.html). You can read more about it at [arxiv.org](https://arxiv.org/abs/1608.06993)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q https://download.pytorch.org/models/densenet161-8d451a50.pth\n",
    "ls *.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Archive the model using the TorchServe model archiver using parameters for the model name, versin, the model file provided by the examples, the serialized densenet model. You can find more information on using the model archiver at [Torch Model Archiver for TorchServe](https://github.com/pytorch/serve/blob/master/model-archiver/README.md). We conclude this step by listing the archived model file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver \\\n",
    "   --model-name densenet161 \\\n",
    "   --version 1.0 \\\n",
    "   --model-file serve/examples/image_classifier/densenet_161/model.py \\\n",
    "   --serialized-file densenet161-8d451a50.pth \\\n",
    "   --extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "   --handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Move the archived model into the model_store subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.mar model_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start TorchServe\n",
    "9. Once you archive and store the model, you will start the TorchServe server in the background. Once it has started, TorchServe listens for your inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "torchserve --start --model-store model_store --models densenet161=densenet161.mar </dev/null &>/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Check status of TorchServe process. The status should read \"Healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Show the model(s) being served."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8081/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "To test the TorchServe model server, you just need to send a request to the prediction API. \n",
    "12. Use curl to download an image. In this case, it is the image of a kitten. We use the -O flag to name the download 'kitten.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Now that you have an image, you use curl to send POST to the TorchServe predict endpoint with the kitten's image. The predictions endpoint returns a prediction response in JSON. With the kitten image, you will see several different types along with the confidence of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T kitten.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. When you have completed running inferences, you may stop the server by executing the torchserve command with the --stop flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchserve --stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
