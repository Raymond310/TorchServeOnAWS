{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With TorchServe\n",
    "\n",
    "This introductory lab will take you through the following hands-on exercises:\n",
    "* [Install TorchServe](https://github.com/pytorch/serve#install-torchserve) and it's dependencies on an Amazon SageMaker Notebook\n",
    "* Create a model store\n",
    "* Download and use the Torch Model Archiver\n",
    "* Start the TorchServe server\n",
    "* Register/Unregister Models\n",
    "* Perform Inference using TorchServe\n",
    "* Host Multiple Models and Scale Workers\n",
    "* Stop the TorchServe server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the TorchServe environment on SageMaker\n",
    "Let's start off by using [Amazon Corretto](https://aws.amazon.com/corretto/) to install Java 11 dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo rpm --import https://yum.corretto.aws/corretto.key \n",
    "sudo curl -L -o /etc/yum.repos.d/corretto.repo https://yum.corretto.aws/corretto.repo\n",
    "sudo yum install -y java-11-amazon-corretto-devel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the Java 11 version and that ``JAVA_HOME`` is properly set by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version\n",
    "!echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use ``pip`` to install TorchServe and the model archiver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchtext torchvision sentencepiece psutil future\n",
    "!pip install torchserve torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ``/serve`` subdirectory already exists, remove it. And, then clone the TorchServe repository into a new serve subdirectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d \"serve\" ]; then\n",
    "    rm -r -f serve\n",
    "fi\n",
    "git clone https://github.com/pytorch/serve.git serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store archived models, we will need to create a model store. Create the ``model_store`` directory which is ultimately referenced via a parameter by the ``model_archiver`` when you are packaging your model. If the ``model_store`` subdirectory already exists, remove all the files. Else, create a subdirectory in which to store your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d \"model_store\" ]; then\n",
    "    rm -f model_store/*    \n",
    "else\n",
    "    mkdir model_store\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab uses a pre-trained model which helps us focus on the serving. Next, we will download a pre-trained model. The DenseNet model is one of the PyTorch TorchVision [models](https://pytorch.org/docs/stable/torchvision/models.html). You can read more about it at [arxiv.org](https://arxiv.org/abs/1608.06993) or on [Kaggle](https://www.kaggle.com/pytorch/densenet161)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q https://download.pytorch.org/models/densenet161-8d451a50.pth\n",
    "ls *.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model, we will archive it by using the [TorchServe Model Archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md). Note the many different arguments that can be passed including the model name, version, the model file, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, we reference the serialized densenet model we just downloaded\n",
    "!torch-model-archiver \\\n",
    "   --model-name densenet161 \\\n",
    "   --version 1.0 \\\n",
    "   --model-file serve/examples/image_classifier/densenet_161/model.py \\\n",
    "   --serialized-file densenet161-8d451a50.pth \\\n",
    "   --extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "   --handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move the archived model into the ``model_store`` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.mar model_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting TorchServe\n",
    "With the model archived and sitting in our model store, we can now start the TorchServe server in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "torchserve --start --model-store model_store --models densenet161=densenet161.mar </dev/null &>/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md) is listening on port 8080 by default. Now that the server has started, let's run a health check on the TorchServe process. The status from the following endpoint command should should read \"Healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being that we have placed the ``densenet161`` model archive in the model store, it was served as soon as the server started up. We can verify this by calling the [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8081/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "To test the TorchServe model server, you just need to send a request to the Inference API. Let's start by pulling down an image of a [Proboscis Monkey](https://en.wikipedia.org/wiki/Proboscis_monkey) and a [Tiger Beetle](https://en.wikipedia.org/wiki/Tiger_beetle).\n",
    "<img src=\"https://torchserve-workshop.s3.amazonaws.com/proboscis-monkey-tiger-beetle-grouped.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://torchserve-workshop.s3.amazonaws.com/proboscis-monkey.jpg\n",
    "!curl -O https://torchserve-workshop.s3.amazonaws.com/tiger-beetle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a couple images, we can use ``curl`` to send ``POST`` to the TorchServe predict endpoint with our images. The predictions endpoint returns a prediction response in JSON. With both the Proboscis Money and the Tiger Beetle, we see several different prediction types along with their associated confidence scores of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T proboscis-monkey.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T tiger-beetle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting Multiple Models and Scaling Workers\n",
    "TorchServe provides a management API to list registered models, register new models to existing servers, unregistering current models, increasing or decreasing number of workers per model, describing the status of a model, adding versions, and setting default versions. The Management API is listening on port 8081 by default, but you can change the default behavior.\n",
    "\n",
    "Let's start by downloading a new model. For this example, we will use a pre=trained Faster RCNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
    "!torch-model-archiver --model-name fastrcnn --version 1.0 \\\n",
    "--model-file serve/examples/object_detector/fast-rcnn/model.py \\\n",
    "--serialized-file fasterrcnn_resnet50_fpn_coco-258fb6c6.pth \\\n",
    "--handler object_detector \\\n",
    "--extra-files serve/examples/object_detector/index_to_name.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have previously done, let's move the model to the model store and then verify it is in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv fastrcnn.mar model_store\n",
    "!ls -l ./model_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can register the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:8081/models?url=fastrcnn.mar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then query the list of registered models to verify our pre=trained Faster RCNN model is also being served."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://localhost:8081/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's scale workers for our model. By default, a new model has no workers assigned to it, so here we set a minimum number of workers.\n",
    "\n",
    "Note: If your model is hosted on a CPU with many cores then you can easily scale the number of threads higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v -X PUT \"http://localhost:8081/models/fastrcnn?min_worker=2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We updated the workers and can now verify as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://localhost:8081/models/fastrcnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can unregister the model if it no longer needs to be served for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X DELETE http://localhost:8081/models/fastrcnn/\n",
    "!curl -X DELETE http://localhost:8081/models/densenet161/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the model was unregistered by querying the API once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://localhost:8081/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when you have completed running inferences, you may stop the server by executing the torchserve command with the ``--stop`` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchserve --stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "The next step removes files created during this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./scripts/cleanup.sh\n",
    "!./scripts/cleanup.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
