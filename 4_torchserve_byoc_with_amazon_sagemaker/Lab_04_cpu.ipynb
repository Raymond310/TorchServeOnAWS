{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TorchServe with Containers with CPU-based Image\n",
    "This introductory lab will take you through the following hands-on exercises:\n",
    "\n",
    "* [Install TorchServe](https://github.com/pytorch/serve#install-torchserve) and it's dependencies on an Amazon SageMaker Notebook\n",
    "* Create a TorchServe Docker image (sans model)\n",
    "* Create TorchServe Docker image from source\n",
    "* Create torch-model-archiver from container\n",
    "* Start TorchServe using a Docker container\n",
    "* Performing Inference using a TorchServe container\n",
    "* Stop the TorchServe Container\n",
    "* Hints for running TorchServe in a Production Docker Environment \n",
    "* Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the TorchServe environment on SageMaker\n",
    "Install the prerequisites. Most of these are the same setup steps that were conducted in Lab 01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./scripts/prerequisites.sh\n",
    "!./scripts/prerequisites.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TorchServe Docker CPU image\n",
    "Create a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serve/docker\n",
    "DOCKER_BUILDKIT=1 docker build --file Dockerfile -t torchserve:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the container with a TorchServe image on ports 8080 and 8081 exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serve/docker\n",
    "docker run -d --rm -it \\\n",
    "    --name no_model_container \\\n",
    "    -p 8080:8080 \\\n",
    "    -p 8081:8081 \\\n",
    "    pytorch/torchserve:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchServe inference and management APIs are accessible on localhost ports 8080 and 8081. The status from the following endpoint command should should read \"Healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that that this container does not contain a model which one can see via the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8081/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now stop this container in order to demonstrate starting a container including a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the TorchServe container.\n",
    "!docker stop no_model_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TorchServe Docker image from source\n",
    "The following are examples on how to use the build_image.sh script to build Docker images from source to support CPU or GPU inference.\n",
    "\n",
    "To create a Docker image for a specific branch, use the following command:\n",
    "\n",
    "   `./build_image.sh -b <branch_name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serve/docker\n",
    "./build_image.sh -b master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided [docker file](serve/docker/Dockerfile) can be used to build images for both CPU and GPU environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch-model-archiver from container\n",
    "We start by copying a sample script to the docker subdirectory. This script file can be used to start both CPU and GPU based images. You can download this [script file for viewing](../source/start_densenet.sh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./scripts/start_densenet.sh ./serve/docker/.\n",
    "!chmod +x serve/docker/start_densenet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start TorchServe using a Docker container\n",
    "To start TorchServe inside the container with a pre-registered densenet-161 image classification model, use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve/docker/start_densenet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may then query the list of registered models to verify our pre=trained densenet_161 model is also being served."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8081/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference using a TorchServe container\n",
    "To test the TorchServe model server, you just need to send a request to the Inference API. Let's start by pulling down an image of a [Proboscis Monkey](https://en.wikipedia.org/wiki/Proboscis_monkey) and a [Tiger Beetle](https://en.wikipedia.org/wiki/Tiger_beetle).\n",
    "<img src=\"https://torchserve-workshop.s3.amazonaws.com/proboscis-monkey-tiger-beetle-grouped.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://torchserve-workshop.s3.amazonaws.com/proboscis-monkey.jpg\n",
    "!curl -O https://torchserve-workshop.s3.amazonaws.com/tiger-beetle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two images, we can use curl to send POST to the TorchServe predict endpoint with our images. The predictions endpoint returns a prediction response in JSON. With both the Proboscis Money and the Tiger Beetle, we see several different prediction types along with their associated confidence scores of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T proboscis-monkey.jpg\n",
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T tiger-beetle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that these results are the same as seen with Lab 01 but here were are using a container rather than SageMaker local mode.\n",
    "\n",
    "## Stop the TorchServe Container\n",
    "Now, shut down the TorchServe container using this command: `docker container stop <container_name | contain id>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop torchserve_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints for running TorchServe in a Production Docker Environment\n",
    "You may wish to consider these options when deploying TorchServe with Docker in a production environment.\n",
    "- Shared Memory Size: The `shm-size` parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n",
    "  \n",
    "\n",
    "- User Limits for System Resources\n",
    "   - `--ulimit memlock=-1`: Maximum locked-in-memory address space.\n",
    "   - `--ulimit stack`: Linux stack size\n",
    "   \n",
    "    The current ulimit values can be viewed by executing ulimit -a. A more exhaustive set of options for resource constraining can be found in the Docker Documentation:\n",
    "    - [Resource constraints](https://docs.docker.com/config/containers/resource_constraints/),\n",
    "    - [Set ulimits in a container](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit), and\n",
    "    - [Runtime constraints](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n",
    "    \n",
    "\n",
    "- Exposing specific ports / volumes between the host & docker env.\n",
    "\n",
    "    - `-p8080:p8080 -p8081:8081` TorchServe uses default ports 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n",
    "    - The model store is passed to torchserve with the --model-store option. You may want to consider using a shared volume if you prefer pre populating models in model-store directory.\n",
    "    \n",
    "    \n",
    "Here is an example using all of these options for production use,\n",
    "\n",
    "```\n",
    "docker run -d --rm \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -p8080:8080 \\\n",
    "    -p8081:8081 \\\n",
    "    --mount type=bind,source=/path/to/model/store,target=/tmp/models <container> torchserve \n",
    "    --model-store=/tmp/models \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "The next step removes files created during this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./scripts/cleanup.sh\n",
    "!./scripts/cleanup.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
