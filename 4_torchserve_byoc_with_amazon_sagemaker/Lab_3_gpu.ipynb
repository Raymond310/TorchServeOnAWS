{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TorchServe with Containers with GPU-based Image\n",
    "#### Note: before you start the GPU portion of this lab, run the follow command and be sure the result is 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() & torch.cuda.device_count() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the above result was 'True', you may continue with this lab.\n",
    "\n",
    "This introductory lab will take you through the following hands-on exercises:\n",
    "\n",
    "* [Install TorchServe](https://github.com/pytorch/serve#install-torchserve) and it's dependencies on an Amazon SageMaker Notebook\n",
    "* Create a TorchServe Docker image (sans model)\n",
    "* Create TorchServe Docker image from source\n",
    "* Create torch-model-archiver from container\n",
    "* Start TorchServe using a Docker container\n",
    "* Performing Inference using a TorchServe container\n",
    "* Stop the TorchServe Container\n",
    "* Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the TorchServe environment on SageMaker\n",
    "Install the prerequisites. Most of these are the same setup steps that were conducted in Lab 01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./scripts/prerequisites.sh\n",
    "!./scripts/prerequisites.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TorchServe Docker GPU image\n",
    "Create a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serve/docker\n",
    "DOCKER_BUILDKIT=1 docker build \\\n",
    "    --file Dockerfile \\\n",
    "    --build-arg BASE_IMAGE=nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 \\\n",
    "    -t torchserve:gpu-latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the container with a TorchServe image on ports 8080 and 8081 exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serve/docker\n",
    "docker run -d --rm -it \\\n",
    "    --name no_model_container \\\n",
    "    --gpus all \\\n",
    "    -p 8080:8080 \\\n",
    "    -p 8081:8081 \\\n",
    "    torchserve:gpu-latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchServe inference and management APIs are accessible on localhost ports 8080 and 8081. The status from the following endpoint command should should read \"Healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that that this container does not contain a model which one can see via the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8081/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now stop this container in order to demonstrate starting a container including a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the TorchServe container.\n",
    "!docker stop no_model_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TorchServe Docker image from source\n",
    "The following are examples on how to use the build_image.sh script to build Docker images from source to support CPU or GPU inference.\n",
    "\n",
    "To create a Docker image for a specific branch, use the following command:\n",
    "\n",
    "   `./build_image.sh -b <branch_name> -g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serve/docker\n",
    "./build_image.sh -b master -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided [docker file](serve/docker/Dockerfile) can be used to build images for both CPU and GPU environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch-model-archiver from container\n",
    "We start by copying a sample script to the docker subdirectory. This script file can be used to start both CPU and GPU based images. You can download this [script file for viewing](../source/start_densenet.sh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./scripts/start_densenet.sh ./serve/docker/.\n",
    "!chmod +x serve/docker/start_densenet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start TorchServe using a Docker container\n",
    "To start TorchServe inside the container with a pre-registered densenet-161 image classification model, use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve/docker/start_densenet.sh --gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may then query the list of registered models to verify our pre=trained densenet_161 model is also being served."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8081/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference using a TorchServe container\n",
    "To test the TorchServe model server, you just need to send a request to the Inference API. Let's start by pulling down an image of a [Proboscis Monkey](https://en.wikipedia.org/wiki/Proboscis_monkey) and a [Tiger Beetle](https://en.wikipedia.org/wiki/Tiger_beetle).\n",
    "<img src=\"https://torchserve-workshop.s3.amazonaws.com/proboscis-monkey-tiger-beetle-grouped.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://torchserve-workshop.s3.amazonaws.com/proboscis-monkey.jpg\n",
    "!curl -O https://torchserve-workshop.s3.amazonaws.com/tiger-beetle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two images, we can use curl to send POST to the TorchServe predict endpoint with our images. The predictions endpoint returns a prediction response in JSON. With both the Proboscis Money and the Tiger Beetle, we see several different prediction types along with their associated confidence scores of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T proboscis-monkey.jpg\n",
    "!curl -X POST http://localhost:8080/predictions/densenet161 -T tiger-beetle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that these results are the same as seen with Lab 01 but here were are using a container rather than SageMaker local mode.\n",
    "\n",
    "## Stop the TorchServe Container\n",
    "Now, shut down the TorchServe container using this command: `docker container stop <container_name | contain id>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop torchserve_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "The next step removes files created during this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./scripts/cleanup.sh\n",
    "!./scripts/cleanup.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
