{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Sentiment Analysis using Transformers on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data inputs from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-175748383800/data-processing-2020-06-26-21-44-08-917/output/preprocessed/'}\n"
     ]
    }
   ],
   "source": [
    "# Replace with your S3 dataset path\n",
    "inputs = {'train': 's3://sagemaker-us-east-1-175748383800/data-processing-2020-06-26-21-44-08-917/output/preprocessed/'}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "        \"model_name\":\"roberta-base\",\n",
    "        \"data_folder\": '/opt/ml/input/data/train',\n",
    "        \"output_folder\": '/opt/ml/model',\n",
    "        \"epochs\": 2,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 64,\n",
    "        \"seed\": 42,\n",
    "        \"max_len\": 160\n",
    "    }\n",
    "\n",
    "metric_definitions = [{'Name': 'validation_accuracy',\n",
    "                       'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorchEstimator(\n",
    "    entry_point='run_training.py',\n",
    "    source_dir='source_dir',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    train_volume_size=50,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    framework_version='1.5.0',\n",
    "    py_version='py3',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 16:37:41 Starting - Starting the training job...\n",
      "2020-06-28 16:37:47 Starting - Launching requested ML instances......\n",
      "2020-06-28 16:39:02 Starting - Preparing the instances for training......\n",
      "2020-06-28 16:40:06 Downloading - Downloading input data...\n",
      "2020-06-28 16:40:30 Training - Downloading the training image.........\n",
      "2020-06-28 16:41:57 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-28 16:41:58,066 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-28 16:41:58,090 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-28 16:42:01,127 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-28 16:42:01,459 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpums308oz/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (20.3)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.6.8-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (4.42.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (0.14.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker-roberta-example, sacremoses\n",
      "  Building wheel for sagemaker-roberta-example (setup.py): started\n",
      "  Building wheel for sagemaker-roberta-example (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-roberta-example: filename=sagemaker_roberta_example-1.0-py3-none-any.whl size=1175 sha256=a192b8da32a4ac3427646788f95d326192916df69fc4f562e113a1f65d693f9c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-is8uzxq2/wheels/03/c7/b1/a46246d9bfe1672fea5781bfefd17167f1dc99a946b525e1a6\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=2b19c3e1f12bd171cbc00fd428da1d2b46c403c29066b8489ccce2fd825d848e\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker-roberta-example sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, regex, filelock, sacremoses, tokenizers, transformers, sagemaker-roberta-example\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.0.12 regex-2020.6.8 sacremoses-0.0.43 sagemaker-roberta-example-1.0 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-06-28 16:42:07,292 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (4.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (2020.6.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (0.0.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker-roberta-example\n",
      "  Building wheel for sagemaker-roberta-example (setup.py): started\n",
      "  Building wheel for sagemaker-roberta-example (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-roberta-example: filename=sagemaker_roberta_example-1.0-py3-none-any.whl size=1175 sha256=dce026f2b1548da57907ddeab9cda47b97f0d05b45c3633d722cd75d00c9ba06\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-x6jrqdi5/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker-roberta-example\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-roberta-example\n",
      "  Attempting uninstall: sagemaker-roberta-example\n",
      "    Found existing installation: sagemaker-roberta-example 1.0\n",
      "    Uninstalling sagemaker-roberta-example-1.0:\n",
      "      Successfully uninstalled sagemaker-roberta-example-1.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-roberta-example-1.0\u001b[0m\n",
      "\u001b[34m2020-06-28 16:42:09,128 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 64,\n",
      "        \"seed\": 42,\n",
      "        \"data_folder\": \"/opt/ml/input/data/train\",\n",
      "        \"max_len\": 160,\n",
      "        \"output_folder\": \"/opt/ml/model\",\n",
      "        \"model_name\": \"roberta-base\",\n",
      "        \"epochs\": 2,\n",
      "        \"learning_rate\": 2e-05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-06-28-16-37-40-902\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-175748383800/pytorch-training-2020-06-28-16-37-40-902/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_training\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_training.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":64,\"data_folder\":\"/opt/ml/input/data/train\",\"epochs\":2,\"learning_rate\":2e-05,\"max_len\":160,\"model_name\":\"roberta-base\",\"output_folder\":\"/opt/ml/model\",\"seed\":42}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_training.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_training\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-175748383800/pytorch-training-2020-06-28-16-37-40-902/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":64,\"data_folder\":\"/opt/ml/input/data/train\",\"epochs\":2,\"learning_rate\":2e-05,\"max_len\":160,\"model_name\":\"roberta-base\",\"output_folder\":\"/opt/ml/model\",\"seed\":42},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-06-28-16-37-40-902\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-175748383800/pytorch-training-2020-06-28-16-37-40-902/source/sourcedir.tar.gz\",\"module_name\":\"run_training\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_training.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--data_folder\",\"/opt/ml/input/data/train\",\"--epochs\",\"2\",\"--learning_rate\",\"2e-05\",\"--max_len\",\"160\",\"--model_name\",\"roberta-base\",\"--output_folder\",\"/opt/ml/model\",\"--seed\",\"42\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_FOLDER=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEN=160\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_FOLDER=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m run_training --batch_size 64 --data_folder /opt/ml/input/data/train --epochs 2 --learning_rate 2e-05 --max_len 160 --model_name roberta-base --output_folder /opt/ml/model --seed 42\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch 1/2\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:32.666 algo-1:63 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:32.666 algo-1:63 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:32.666 algo-1:63 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:32.693 algo-1:63 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:32.694 algo-1:63 INFO hook.py:422] Hook is writing from the hook with pid: 63\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.567 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.568 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.568 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.568 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.575 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.575 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.575 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.579 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.579 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.579 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.580 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.582 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.582 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.582 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.586 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.586 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.586 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.587 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.589 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.589 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.589 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.593 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.593 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.594 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.594 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.596 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.596 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.597 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.601 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.601 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.601 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.601 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.604 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.604 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.604 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.608 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.608 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.608 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.609 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.611 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.611 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.611 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.615 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.615 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.615 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.616 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.618 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.618 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.618 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.623 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.623 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.623 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.623 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.626 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.626 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.626 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.630 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.630 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.630 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.631 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.633 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.633 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.633 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.637 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.637 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.638 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.638 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.640 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.640 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.641 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.645 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.645 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.645 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.646 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.648 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.648 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.648 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.652 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.652 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.652 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.653 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.655 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.655 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:42:33.655 algo-1:63 WARNING hook.py:870] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11 NoneType\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mtrain_loss: 0.714739048266196\u001b[0m\n",
      "\u001b[34mtrain_accuracy: 0.6741232093712511\u001b[0m\n",
      "\u001b[34mval_loss: 0.6011444582388952\u001b[0m\n",
      "\u001b[34mval_accuracy: 0.7382465057179162\n",
      "\u001b[0m\n",
      "\u001b[34mEpoch 2/2\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mtrain_loss: 0.5289136932508366\u001b[0m\n",
      "\u001b[34mtrain_accuracy: 0.7782795850680968\u001b[0m\n",
      "\u001b[34mval_loss: 0.5612198825065906\u001b[0m\n",
      "\u001b[34mval_accuracy: 0.7598475222363406\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-06-28 16:46:30.320 algo-1:63 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-06-28 16:46:30,867 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-06-28 16:47:00 Uploading - Uploading generated training model\n",
      "2020-06-28 16:48:42 Completed - Training job completed\n",
      "Training seconds: 516\n",
      "Billable seconds: 516\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
